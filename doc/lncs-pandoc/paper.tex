% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={HR Analytics: the wakening},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[3] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc} % for \widthof, \maxof
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\maxof{\widthof{#1}}{\csllabelwidth}}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{\textbf{HR Analytics: the wakening}}
\author{true \and true}
\date{}

\begin{document}
\maketitle
\begin{abstract}
The size of companies has seen an exponential growth over the years.
Corporations recruit anywhere from a few hundred to a few thousand
employees every year. With such rates, HR management in companies is
proving to be more and more significant every day. Done manually,
managing human resources is a laborious task, given the sheer quantity
of employees. Luckily, over the years, data analytics in HR is emerging
as an integral part in corporate operation. Yet, there remain a few
problems that involve human involvement, one of them being selecting
candidates that are eligible for a promotion. This paper proposes a
solution involving machine learning techniques such as the Random Forest
and XGBoost algorithms to learn from past employee records to aid this
decision making process.
\end{abstract}

(Abdul and Mohammed 2019) \# Introduction

HR analytics in companies plays a major role in restructuring the
operand of their HR department. A company of sizeable proportions deals
with hundreds of employee records every day. Although HR analytics has
been in operation in companies for years, some of these operations are
still done manually. Automating such processes will aid in saving
valuable time and increasing overall efficiency in the operation of the
company. Eligibility for promotions depends on many different criteria.
These criteria vary between companies and even between departments
within a company. Currently, there is no definitive method to determine
why an employee receives a promotion since such decisions require
logical reasoning and understanding the current environmental factors
that computers are just not capable of. Problems such as this are where
we utilize machine learning to our advantage. Machine learning has been
used to solve similar problems in different domains for several years
now. In areas where some kind of human intervention is necessary, a well
trained machine learning algorithm has been proven to be an acceptable
substitute, if not an ideal solution. Here, we will be using machine
learning techniques to not only predict the promotion status of future
employees, but to also determine which of the provided attributes from
the employees' data is most relevant to making this prediction.

\hypertarget{literature-survey}{%
\section{Literature Survey}\label{literature-survey}}

Machine learning, as the name suggests, is the field of computing that
deals with algorithms and concepts that attempt to emulate specified
human behavior {[}1{]}. First pioneered in the late 1950s, machine
learning has come a long way over the years to the point where we can
now depend on them to perform reliably even in production level
environments. The proposed solution to the promotion problem here is to
train a machine learning algorithms to analyze past employee records of
employees who were shortlisted for a promotion. These records include
information about the employee, along with their promotion status, that
describes whether they were promoted or not. This method of training a
machine learning algorithm is called Supervised Learning {[}2{]}, where
the final result expected from the algorithm is provided. This is the
approach that is going to be used here. Supervised learning algorithms
generally deal with problems that have definitive solutions to them,
such as classifying an item as belonging to one of multiple categories.
The specified algorithms being proposed in this paper are namely, the
Random Forest and XGBoost algorithms {[}3{]}.

\hypertarget{decision-tree}{%
\subsection{Decision Tree}\label{decision-tree}}

A decision tree, as the name suggests, is a graphical representation of
different attributes of any given data in a tree like structures. It
attempts to map out the correlation between the combinations of values
of each different attribute to the overall outcome. A decision tree is
represented as a flowchart, where each node represents an attribute of
the data and the connections emerging from that node are the possible
values that can be assigned to that attribute {[}4{]}. The edges present
in the tree are seen as different pathways that can be taken to make a
prediction, based on the provided values are.

\hypertarget{random-forest}{%
\subsection{Random Forest}\label{random-forest}}

As useful as decision trees are, they fall short as problems become more
and more complex. This is attributed to the fact that they are quite
unstable, as even small changes in the data can affect the structure of
the tree profoundly. This causes over fitting, which means that the tree
cannot perform as well on real world data as it does on its training
data. The Random Forest algorithm aims to overcome that shortcoming.
Rather than have a single tree making a definitive decision, it utilizes
multiple trees, each making a prediction to arrive at its final
conclusion. This algorithm uses the drawback of the decision tree to its
advantage to create diverse trees by altering the data little by little.
This helps keep the over fitting problem in check. Finally, the
predictions from all the decision trees are polled and the prediction
with the highest frequency is taken as the final prediction.

\hypertarget{xgboost}{%
\subsection{XGBoost}\label{xgboost}}

The XGBoost (short for Extreme Gradient Boosting) algorithm is
essentially a decision tree which utilizes the concept of gradient
boosting to enhance the performance of the decision tree. This
algorithm performs best when being used to model small to medium sized
structured data. The XGBoost algorithm {[}6{]} is generally preferred
over other conventional algorithms as it combines many different traits
from other algorithms, such as bagging, gradient boosting, random forest
and decision trees and makes improves upon them through system
optimization and algorithmic enhancements such as regularization,
sparsity awareness, weighted quartile sketch, and so on.

\hypertarget{gradient-boosting}{%
\subsubsection{Gradient Boosting}\label{gradient-boosting}}

Gradient boosting is a repetitive algorithm used to leverage the
patterns of mistakes made by a model to strengthen the model using weak
predictions. Basically, the data is modeled very simply and is analyzed
for errors to identify data points that are difficult for the model to
fit. The model is tweaked to fit better for those particular data
points. Finally, this is combined with the original models for an
optimal solution.

\hypertarget{methodology}{%
\section{Methodology}\label{methodology}}

\hypertarget{dataset}{%
\subsection{Dataset}\label{dataset}}

To fully explain the approach to solving this problem, we must first
analyze and understand the dataset. The dataset comprises of data
collected by a company on previous candidates who were shortlisted for a
promotion, which is collected from the Kaggle website. The first 10 rows
of the data are shown in the Figure 1. This dataset consists of 14
different attributes or columns, with 54808 total observations or rows.

\begin{itemize}
\tightlist
\item
  \textbf{employee\_id(int)}: Unique id of the employee.
\item
  \textbf{department(string)}: The department to which the employee
  belongs to. Possible values are: `Analytics,' `Finance' , `HR' ,
  `Legal' , `Operations,' `Procurement,' `R\&D,' `Sales \& Marketing,'
  `Technology.'
\item
  \textbf{region(string)}: Region of employment. Possible values are:
  `region\_10,' `region\_11,' `region\_12,' `region\_13,' `region\_14,'
  `region\_15,' `region\_16,' `region\_17,' `region\_18,' `region\_19,'
  `region\_2,' `region\_20,' `region\_21,' `region\_22,' `region\_23,'
  `region\_24,' `region\_25,' `region\_26,' `region\_27,' `region\_28,'
  `region\_29,' `region\_3,' `region\_30,' `region\_31,' `region\_32,'
  `region\_33,' `region\_34,' `region\_4,' `region\_5,' `region\_6,'
  `region\_7,' `region\_8,' `region\_9.'
\item
  \textbf{education(string)}: Describes the level of education of the
  employee. Possible values are: `Bachelor's,' `Below Secondary,'
  `Master's \& above.'
\item
  \textbf{gender(string)}: Gender of the employee. Possible values are:
  `f,' `m.'
\item
  \textbf{recruitment\_channel(string)}: Channel of recruitment of the
  employee. Possible values are: `referred,' `sourcing,' `other.'
\item
  \textbf{no\_of trainings(int)}: Describes the number of training
  programs completed by the employee. Range: from 1 to 10.
\item
  \textbf{age(int)}: Describes the age of the employee.
  previous\_year\_rating (int): Employee rating from previous year.
  Range from 1 to 5.
\item
  \textbf{length\_of\_service(int)}: The service length of the employee
  in years.
\item
  \textbf{KPIs\_met \textgreater80\%(int)}: Describes whether the
  employee's Key Performance Indicators score are greater than 80\%.
  Value is 1 if yes, else 0.
\item
  \textbf{awards\_won?(int)}: Whether the employee won any awards last
  year. Value is 1 if yes, else 0.
\item
  \textbf{avg\_training\_score(int)}: Employee's average training score
  in training evaluations.
\item
  \textbf{is\_promoted(int)}: Whether the employee was promoted or not.
  Value is 1 if yes, else 0.
\end{itemize}

Here, the promoted variable is our target variable. This is what we have
to predict to produce our final result.

\hypertarget{feature-engineering}{%
\subsection{Feature Engineering}\label{feature-engineering}}

First off, we remove the employee\_id column as it is just a column to
distinguish records by and will not realistically impact the decision of
whether an employee is to be promoted or not. This brings down our
variable count to 13.

\hypertarget{duplicate-values}{%
\subsubsection{Duplicate Values}\label{duplicate-values}}

Even though the size of our dataset is huge, we cannot be sure that all
the observations in the data are unique. Looking at our data, we find
that there are 118 duplicate records that are present in our dataset.
Even though it does not seem like a significantly large number,
duplicate records will negatively impact the training process of machine
learning algorithms and may cause them to over-fit. After the removal of
the duplicate values, we have 54690 observations left.

\hypertarget{null-values}{%
\subsubsection{Null Values}\label{null-values}}

Looking at our dataset, we see there are missing values. Observations
with such values are not suitable for machine learning algorithms.
Luckily, out of 54690 observations, we have about 4042 observations
with missing values. That accounts to about 7\% of our total data. A
plot of this can be seen in Figure 2.

This plot shows the frequency of null values in each column. There are
only two columns with null values in our data, education and
previous\_year\_rating, with 2398 and 4062 records with missing values
respectively. These can be dealt with in one of two ways. Either, these
observations could be completely removed or they could be systematically
generated using existing data. We choose the latter, as with that
approach, we still maintain the size of our dataset. Since both of these
attributes are categorical, we can fill in missing values by finding the
mode of the values that belong to that attribute. We also take into
account the value of the target class for each attribute.

\begin{figure}
\centering
\includegraphics{./imgs/null_freq.png}
\caption{Bar plot of columns along with the frequence of null values in
them}
\end{figure}

\hypertarget{preprocessing}{%
\subsection{Preprocessing}\label{preprocessing}}

The data is pre-processed to make it suitable for our algorithms. First,
the target column values are renamed from ``0'' and ``1'' to ``no'' and
``yes'' so that the algorithm recognizes as categorical variables. Then,
a dummy variable are generated from the values of all the categorical
attributes, or performs one-hot encoding for all the attributes.

\hypertarget{class-imbalance-problem}{%
\subsection{Class imbalance problem}\label{class-imbalance-problem}}

In this dataset, there is a clear imbalance between the values of our
target variables. Out of 54690 observations, only 4665 observations are
of class ``no,'' while 50025 observations are of class ``yes.'' This is
a significant issue, as the difference exceeds more than 50\% of the
data. For a machine learning algorithm to be properly able to parse and
understand the given data, there should ideally be an equal distribution
of the number of examples with the different classes it is meant to
classify other data into. When one class takes precedence over the other
class in the dataset, the algorithm is less likely to learn what the
properties of each class are and tends to forget the less frequent
class's properties all together during training.

\begin{figure}
\centering
\includegraphics{./imgs/class_imbalance.png}
\caption{Distribution of class percentage before under-sampling}
\end{figure}

To ensure that this does not happen, we must make sure that there are
equal numbers of examples for both the cases. Here, we simply randomly
sample a subset of the data where the class is ``yes,'' as it is the
class with higher frequency and append it to the observations with the
class ``no'' to generate a new, minified training set with equal number
of ``yes'' and ``no'' observations. This process is called under
sampling. This brings down the size of our dataset to 9330 observations
total. Even though it is only a fraction of the original 54690, it is
still a significant amount and should be enough to train our algorithms
along with being balanced. Figure 3 and 4 shows the distribution of
class percentage before and after under sampling.

\begin{figure}
\centering
\includegraphics{./imgs/class_balance.png}
\caption{Distribution of class percentage after under-sampling}
\end{figure}

\hypertarget{training-and-evaluation}{%
\subsection{Training and Evaluation}\label{training-and-evaluation}}

The dataset is split into a training set and a testing set where the
testing set contains 1/5ths of the records in the dataset and the rest
belongs to the training set. The chosen method of evaluation to measure
the performance of our models is K-fold cross validation, where K is
taken to be

\hypertarget{experimental-results}{%
\section{Experimental Results}\label{experimental-results}}

\hypertarget{results-on-random-forest}{%
\subsection{Results on Random Forest}\label{results-on-random-forest}}

The confusion matrix of the model evaluated on the training set, with 5
fold cross validation is as follows:

\begin{verbatim}
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   no  yes
       no  37.5  5.8
       yes 12.5 44. 
                            
 Accuracy (average) : 0.8162

\end{verbatim}

The confusion matrix on the model evaluated on the test set:

\begin{verbatim}
Random Forest 

7464 samples
  12 predictor
   2 classes: 'no', 'yes' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 5972, 5972, 5972, 5970, 5970 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.7600484  0.5200968
  28    0.8161811  0.6323622
  54    0.8105528  0.6211057

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was mtry = 28.
\end{verbatim}

We can see that the performance of the model is consistent over both the
training and testing set. So, we can ensure that our model has not
over-fit. The random forest model also helps us calculate the importance
of each attribute. The attribute importance scores, as predicted by our
trained random forest model are shown in the Table 1 and Figure 5.

\begin{longtable}[]{@{}ll@{}}
\caption{Top 20 important features obtained for Random Forest with no
scaling of scores.}\tabularnewline
\toprule
Attribute & Overall\tabularnewline
\midrule
\endfirsthead
\toprule
Attribute & Overall\tabularnewline
\midrule
\endhead
avg\_training\_score & 832.70\tabularnewline
KPIs\_met..80. & 533.48\tabularnewline
previous\_year\_rating & 381.64\tabularnewline
length\_of\_service & 313.44\tabularnewline
age & 301.19\tabularnewline
departmentSales \& Marketing & 186.40\tabularnewline
awards\_won. & 114.01\tabularnewline
departmentOperations & 102.15\tabularnewline
departmentProcurement & 67.42\tabularnewline
no\_of\_trainings & 67. 8\tabularnewline
recruitment\_channelsourcing & 60.31\tabularnewline
genderm & 55.79\tabularnewline
regionregion\_2 & 41.94\tabularnewline
departmentTechnology & 39.70\tabularnewline
departmentFinance & 38.99\tabularnewline
regionregion\_22 & 38.76\tabularnewline
regionregion\_7 & 36.84\tabularnewline
educationMaster's \& above & 31. 7\tabularnewline
departmentHR & 30.92\tabularnewline
educationBachelor's & 30.88\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{./imgs/rf_feat_imp.png}
\caption{Plot of feature importance according to Random Forest}
\end{figure}

We can see here that the model has rated the average\_training\_score,
KPI\_mets \textgreater{} 80 and the previous\_year\_rating as the 3 most
important attributes for this decision.

\hypertarget{results-on-xgboost}{%
\subsection{Results on XGBoost}\label{results-on-xgboost}}

Confusion matrix for the training set, evaluated with 5 fold cross
validation is as follows:

\begin{verbatim}
Cross-Validated (5 fold) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction   no  yes
       no  38.1  5.1
       yes 11.9 44.9
\end{verbatim}

Confusion matrix for the test set:

\begin{verbatim}
Confusion Matrix and Statistics

        
xgbpreds  no yes
     no  731 108
     yes 202 825
                                          
               Accuracy : 0.8339          
                 95% CI : (0.8162, 0.8505)
    No Information Rate : 0.5             
    P-Value [Acc > NIR] : < 2. e-16       
                                          
                  Kappa : 0.6677          
                                          
 Mcnemar's Test P-Value : 1. 77e-07       
                                          
            Sensitivity : 0.7835          
            Specificity : 0.8842          
         Pos Pred Value : 0.8713          
         Neg Pred Value : 0.8033          
             Prevalence : 0.5000          
         Detection Rate : 0.3917          
   Detection Prevalence : 0.4496          
      Balanced Accuracy : 0.8339          
                                          
       'Positive' Class : no
\end{verbatim}

The XGBoost model gives us a final validation accuracy of 0.8339 (or
83.38\%), which is marginally higher when compared to the random forest.
Other statistics such as the specificity and the sensitivity are also
higher, which tells us that the XGBoost model will give us a better
overall performance. The feature importance as calculated by the XGBoost
tree is shown in Table 2 and Figure 6.

\begin{longtable}[]{@{}ll@{}}
\caption{Top 20 important features obtained for XGBoost without scaling
scores.}\tabularnewline
\toprule
Attribute & Overall\tabularnewline
\midrule
\endfirsthead
\toprule
Attribute & Overall\tabularnewline
\midrule
\endhead
avg\_training\_score & 0.98036\tabularnewline
KPIs\_met..80. & 0.39061\tabularnewline
previous\_year\_rating & 0.140940\tabularnewline
departmentSales \& Marketing & 0.069869\tabularnewline
length\_of\_service & 0.048764\tabularnewline
awards\_won. & 0.034820\tabularnewline
departmentProcurement & 0.031456\tabularnewline
departmentOperations & 0.029417\tabularnewline
age & 0.018234\tabularnewline
departmentHR & 0.011814\tabularnewline
departmentFinance & 0.011585\tabularnewline
departmentTechnology & 0.010275\tabularnewline
no\_of\_trainings & 0.007006\tabularnewline
regionregion\_22 & 0.006087\tabularnewline
regionregion\_4 & 0.003136\tabularnewline
genderm & 0.002850\tabularnewline
departmentR\&D & 0.002684\tabularnewline
departmentLegal & 0.002543\tabularnewline
educationMaster's \& above & 0.002407\tabularnewline
regionregion\_2 & 0.002385\tabularnewline
\bottomrule
\end{longtable}

\begin{figure}
\centering
\includegraphics{./imgs/xb_feat_imp.png}
\caption{Top 20 important features obtained for XGBoost without scaling
scores}
\end{figure}

We notice that the list of the 10 most important features according to
both the Random Forest and XGBoost models are nearly identical with the
exception of no\_of\_trainings and departmentHR. This tells us that the
other attributes can be discarded to get improved performance from the
models.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

As we have demonstrated above, the use of machine learning as a
predictive decision making tool or at least a suggestive tool is a
completely viable solution for the presented problem. With fairly
limited data, we were able to train algorithms to perform with
significantly good accuracy. We can improve upon this with more data and
more optimized solutions. Thus, we conclude that machine learning in the
field of HR analytics by reducing the amount of time that goes into
decision making, thereby increasing work efficiency.

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-vishal}{}%
Abdul, Quddus, and Abdulquddus Mohammed. 2019. {``HR ANALYTICS: A MODERN
TOOL IN HR FOR PREDICTIVE DECISION MAKING.''} \emph{Journal of
Management} 6 (May): 51--63.
\url{https://doi.org/10.34218/JOM.6.3.2019.007}.

\end{CSLReferences}

\end{document}
